input {
  # Beats input (for Filebeat)
  beats {
    port => 5044
    host => "0.0.0.0"
  }
  
  # Direct HTTP input for applications
  http {
    port => 8080
    host => "0.0.0.0"
    codec => json
    additional_codecs => {
      "application/json" => "json"
    }
    response_headers => {
      "Access-Control-Allow-Origin" => "*"
      "Access-Control-Allow-Methods" => "GET, POST, OPTIONS"
      "Access-Control-Allow-Headers" => "Content-Type"
    }
  }
  
  # Redis input for application logs
  redis {
    host => "redis"
    port => 6379
    key => "logstash:logs"
    data_type => "list"
    codec => json
  }
}

filter {
  # Add common fields
  mutate {
    add_field => { "environment" => "${ENVIRONMENT:production}" }
    add_field => { "cluster" => "${CLUSTER_NAME:eduhub}" }
  }
  
  # Parse application logs
  if [fields][service] {
    mutate {
      add_field => { "service_name" => "%{[fields][service]}" }
    }
  }
  
  # Parse NestJS logs
  if [message] =~ /^\[.*\] \d+/ {
    grok {
      match => { 
        "message" => "\[%{DATA:context}\] %{NUMBER:process_id}%{SPACE}-%{SPACE}%{TIMESTAMP_ISO8601:log_timestamp} %{SPACE}%{LOGLEVEL:level} \[%{DATA:logger}\] %{GREEDYDATA:log_message}"
      }
      add_field => { "log_type" => "nestjs" }
    }
    
    date {
      match => [ "log_timestamp", "yyyy-MM-dd HH:mm:ss,SSS" ]
    }
  }
  
  # Parse Nginx access logs
  if [fields][log_type] == "nginx_access" {
    grok {
      match => {
        "message" => '%{IPORHOST:remote_addr} - %{DATA:remote_user} \[%{HTTPDATE:time_local}\] "%{WORD:method} %{DATA:request} HTTP/%{NUMBER:http_version}" %{INT:status} %{INT:body_bytes_sent} "%{DATA:http_referer}" "%{DATA:http_user_agent}" "%{DATA:http_x_forwarded_for}" rt=%{NUMBER:request_time} uct="%{DATA:upstream_connect_time}" uht="%{DATA:upstream_header_time}" urt="%{DATA:upstream_response_time}"'
      }
      add_field => { "log_type" => "nginx_access" }
    }
    
    date {
      match => [ "time_local", "dd/MMM/yyyy:HH:mm:ss Z" ]
    }
    
    # Convert numeric fields
    mutate {
      convert => { 
        "status" => "integer"
        "body_bytes_sent" => "integer"
        "request_time" => "float"
      }
    }
    
    # Add response time categories
    if [request_time] {
      if [request_time] < 0.1 {
        mutate { add_field => { "response_time_category" => "fast" } }
      } else if [request_time] < 1.0 {
        mutate { add_field => { "response_time_category" => "medium" } }
      } else {
        mutate { add_field => { "response_time_category" => "slow" } }
      }
    }
  }
  
  # Parse MySQL slow query logs
  if [fields][log_type] == "mysql_slow" {
    grok {
      match => {
        "message" => "# Time: %{TIMESTAMP_ISO8601:query_timestamp}\s*# User@Host: %{DATA:user}@%{DATA:host} \[%{DATA:host_ip}\]\s*# Thread_id: %{INT:thread_id}\s+Schema: %{DATA:schema}\s+QC_hit: %{DATA:qc_hit}\s*# Query_time: %{NUMBER:query_time}\s+Lock_time: %{NUMBER:lock_time} Rows_sent: %{INT:rows_sent}\s+Rows_examined: %{INT:rows_examined}\s*%{GREEDYDATA:query}"
      }
      add_field => { "log_type" => "mysql_slow_query" }
    }
    
    mutate {
      convert => {
        "query_time" => "float"
        "lock_time" => "float"
        "rows_sent" => "integer"
        "rows_examined" => "integer"
        "thread_id" => "integer"
      }
    }
  }
  
  # Parse Redis logs
  if [fields][log_type] == "redis" {
    grok {
      match => {
        "message" => "%{INT:process_id}:%{CHAR:role} %{MONTHDAY:day} %{MONTH:month} %{TIME:time} %{CHAR:level} %{GREEDYDATA:redis_message}"
      }
      add_field => { "log_type" => "redis" }
    }
  }
  
  # Parse Docker container logs
  if [container][name] {
    mutate {
      add_field => { "container_name" => "%{[container][name]}" }
      add_field => { "log_source" => "docker" }
    }
  }
  
  # Detect error patterns
  if [level] =~ /(?i)error/ or [message] =~ /(?i)error|exception|failed|fatal/ {
    mutate {
      add_field => { "alert_level" => "error" }
      add_field => { "needs_attention" => "true" }
    }
  } else if [level] =~ /(?i)warn/ or [message] =~ /(?i)warn|warning/ {
    mutate {
      add_field => { "alert_level" => "warning" }
    }
  }
  
  # Add geolocation for IP addresses
  if [remote_addr] and [remote_addr] !~ /^(10\.|192\.168\.|172\.(1[6-9]|2[0-9]|3[0-1])\.)/ {
    geoip {
      source => "remote_addr"
      target => "geoip"
    }
  }
  
  # Parse JSON messages
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
      target => "json_data"
    }
  }
  
  # Clean up fields
  mutate {
    remove_field => [ "host", "agent", "ecs", "input", "log" ]
  }
}

output {
  # Output to Elasticsearch with index patterns
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    
    # Dynamic index based on service and date
    index => "eduhub-%{[service_name]:unknown}-%{+YYYY.MM.dd}"
    
    # Document type based on log type
    template_name => "eduhub_logs"
    template => "/usr/share/logstash/templates/eduhub-template.json"
    template_overwrite => true
    
    # Document ID for deduplication
    document_id => "%{[@metadata][fingerprint]}"
  }
  
  # Output errors to a separate index
  if [alert_level] == "error" {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "eduhub-errors-%{+YYYY.MM.dd}"
    }
  }
  
  # Output to stdout for debugging (disable in production)
  # stdout {
  #   codec => rubydebug
  # }
  
  # Optional: Send critical errors to external alerting
  if [alert_level] == "error" and [needs_attention] == "true" {
    http {
      url => "${ALERT_WEBHOOK_URL}"
      http_method => "post"
      content_type => "application/json"
      format => "json"
      mapping => {
        "alert_type" => "application_error"
        "service" => "%{[service_name]}"
        "message" => "%{[message]}"
        "timestamp" => "%{[@timestamp]}"
        "environment" => "%{[environment]}"
      }
    }
  }
}